{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we are building?**\n",
    "\n",
    "Transformer based neural network from scratch using paper *Attention is all you need* which will be train on Shakespeare toy dataset to generate a sequence of **characters** given an input (sequence of characters)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset already downloaded!\n",
      "data/tiny_shakespeare_data.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "dataset_name = \"tiny_shakespeare_data.txt\"\n",
    "\n",
    "dataset_folder = Path(\"data/\")\n",
    "dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = dataset_folder / dataset_name\n",
    "\n",
    "# check to see if dataset already exits and download\n",
    "if not dataset.is_file():\n",
    "    # download the data\n",
    "    print(f\"downloading the dataset!\")\n",
    "    data = requests.get(url=url)\n",
    "    with open(dataset, \"wb\") as f:\n",
    "        f.write(data.content)\n",
    "    print(f\"dataset downloaded!\")\n",
    "else:\n",
    "    print(f\"dataset already downloaded!\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters : 1115394\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "with open(dataset, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# total length\n",
    "print(f\"total characters : {len(text)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all possible characters and vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible characters the model can see and emit\n",
    "chars = sorted(list(set(text)))\n",
    "# number of characters\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Decode (Tokenizers: convert chars to string and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cti = {c:i for i, c in enumerate(chars)}\n",
    "itc = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [cti[i] for i in x]\n",
    "decode = lambda lst: ''.join([itc[i] for i in lst])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and convert entire dataset into tensor and perform train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# tokenize entire dataset\n",
    "data = torch.tensor(encode(text))\n",
    "# train validate split\n",
    "split_pos = int(len(data)*0.9) # 90% for training, 10% for validation\n",
    "train_data = data[:split_pos]\n",
    "val_data = data[split_pos:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch of Inputs and Targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "block size / context length = max length of input to the transformer (can't feed whole dataset, so we give small random chunks)\n",
    "\n",
    "we have to generate batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "block_size = 8 # input length, will generate 8 samples\n",
    "batch_size = 4 # batch of 4 inputs\n",
    "\n",
    "def get_batch(split):\n",
    "    # get data\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # get batch\n",
    "    random_offsets = torch.randint(low=0, high=len(data)-block_size, size=[batch_size])\n",
    "    x = torch.stack([data[i: i+block_size] for i in random_offsets])\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in random_offsets])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"input:\\n{xb}\\ntarget:\\n{yb}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        # lookup table for character embedding vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(input=logits, target=target)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_token):\n",
    "        # generate max_token characters\n",
    "        for i in range(max_token):\n",
    "            logits, loss = self(idx) # (B, T, C)\n",
    "            # pick the last character embedding of each \n",
    "            # sample prediction in the batch\n",
    "            last_logits = logits[:,-1,:] # (B, C) -> (4, 65)\n",
    "            # convert it to a prob distribution\n",
    "            y_prob = F.softmax(input=last_logits, dim=1)\n",
    "            # one sample from each prob distribution\n",
    "            next_idx = torch.multinomial(input=y_prob, num_samples=1) # (B, 1)\n",
    "            # concatenate the prediction with given input\n",
    "            idx = torch.cat((idx, next_idx), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "model = BiGram(vocab_size=vocab_size)\n",
    "# logits, loss = model(xb, yb)\n",
    "# print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghani/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3741, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb ,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PFW'd ABun wicay grear car myothed:\n",
      "NEThe TI\n",
      "Qat herthergnend.\n",
      "IZAm;'sinde g wals slulobus whe\n",
      "\n",
      "KIZ!k fer\n",
      "Wod he k\n",
      "MILy,\n",
      "\n",
      "IEThoZELEN:\n",
      "\n",
      "\n",
      "GheayLYk\n",
      "WIIUCIINDerf canthinin: ans nd s\n",
      "\n",
      "HRDXzQzLPUPore t soforn I's temrpeFFI thattersurave d myou ftr forgg o \n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long), 250)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Trick in Self-attention\n",
    "\n",
    "Previously, we were only looking at the last character to predict the sequence, but, now we will start considering the whole context in order to generate new character sequence.\n",
    "\n",
    "To achieve the said objective, we will consider the weighted aggregation of the embedding vectors of all the previous character (context) until and including the character at time t, in order to predict the character at t+1. We will use matrix multiplication to achieve that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 16])\n",
      "tensor([[[0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0929, 0.0087, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0143, 0.0770, 0.1096, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1761, 0.1120, 0.1765, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0932, 0.0522, 0.0975, 0.0126, 0.1079, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3150, 0.4760, 0.2602, 0.1279, 0.3768, 0.0285, 0.0000, 0.0000],\n",
      "         [0.0756, 0.2285, 0.2359, 0.8185, 0.0831, 0.6369, 0.9271, 0.0000],\n",
      "         [0.1929, 0.0456, 0.1203, 0.0097, 0.4322, 0.3347, 0.0729, 1.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# matrix to allow avg sum\n",
    "# a = torch.tril(torch.ones(size=(4,4), dtype=int))\n",
    "# a = a / a.sum(dim=1, keepdim=True)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(torch.matmul(a, b))\n",
    "\n",
    "# technique two (efficient one)\n",
    "B, T, C = (1, 8, 32)\n",
    "x = torch.randn(B, T, C)\n",
    "tril_ones = torch.tril(torch.ones(size=(T, T)))\n",
    "\n",
    "# single head performing self attention \n",
    "HEAD_SIZE = 16 # head size (dimensin of key and query vectors for each token)\n",
    "key_vectors = nn.Linear(C, HEAD_SIZE, bias=False) # (T, HdSz)\n",
    "query_vectors = nn.Linear(C, HEAD_SIZE, bias=False) # (T, HdSz)\n",
    "value_vectors = nn.Linear(C, HEAD_SIZE, bias=False) # (T, HdSz)\n",
    "\n",
    "# generating key, query and value for x (input)\n",
    "key = key_vectors(x) # (B, T, HdSz)\n",
    "query = query_vectors(x) # (B, T, HdSz)\n",
    "value = value_vectors(x) # (B, T, HdSz)\n",
    "\n",
    "# wei is a matrix where each cell is dot product of a query vector & a key vector\n",
    "# rows contain tokens' queries and columns are tokens' key vectors\n",
    "#  |         a         |        b           |     c      ...\n",
    "# a| query(a) . key(a) | query(a) . key(b)  |  query(a) . key(c) ...\n",
    "# b| query(b) . key(a) | query(b) . key(b)  |  query(b) . key(c) ...\n",
    "# c| query(c) . key(a) | query(b) . key(b)  |  query(c) . key(c) ...\n",
    "# ... \n",
    "wei = query @ key.transpose(-2, -1) # (B, T, HdSz) @ (B, HdSz, T) --> (B, T, T)\n",
    "# setting future tokens weights/affinities to -inf\n",
    "wei = wei.masked_fill(tril_ones == 0, float('-inf'))\n",
    "# apply softmax for smooth distribution\n",
    "wei = torch.softmax(wei, dim=1)\n",
    "\n",
    "output = wei @ value # (T, T) @ (B, T, HdSz) --> (B, T, HdSz)\n",
    "print(output.shape)\n",
    "\n",
    "print(wei)\n",
    "\n",
    "# print(x)\n",
    "# bow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
