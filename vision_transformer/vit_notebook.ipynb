{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghani/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from os import cpu_count\n",
    "\n",
    "import engine\n",
    "import data_setup\n",
    "from helper_function import set_seeds, plot_loss_curves\n",
    "from data_download import download_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing directories\n",
    "TRAIN_DIR = \"./data/pizza_steak_sushi/train\"\n",
    "TEST_DIR = \"./data/pizza_steak_sushi/test\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ViT image transformation\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Creates training and testing dataloaders\n",
    "train_dl, test_dl, class_names = data_setup.create_dataloaders(train_dir=TRAIN_DIR,\n",
    "                                                    test_dir=TEST_DIR,\n",
    "                                                    transform=simple_transform,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch of img, labels\n",
    "# img_batch, label_batch = next(iter(train_dl))\n",
    "# # single image\n",
    "# img, label = img_batch[0], label_batch[0]\n",
    "# img_permute = img.permute(1,2,0)\n",
    "\n",
    "# plt.imshow(img_permute)\n",
    "# plt.title(f\"Class is {class_names[label]}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## experimenting to see patch inputs, outputs and patches\n",
    "\n",
    "# # Get image dimensions and patches\n",
    "# H, W, C = 224, 224, 3 #img_permute.shape\n",
    "# patch_size = 16 #patch dimension\n",
    "# N = H*W // patch_size**2 # total number of patches\n",
    "# assert H*W % patch_size**2 == 0, \"Image must be divided into equal patches\"\n",
    "# # print(f\"The sequence of patches has length {N}\")\n",
    "# D = 768 # P**2 * C\n",
    "\n",
    "# # G\n",
    "# patch_per_row = H // patch_size\n",
    "# patch_per_col = W // patch_size\n",
    "\n",
    "# # Create a series of subplots\n",
    "# fig, axs = plt.subplots(nrows=patch_per_row,\n",
    "#                         ncols=patch_per_col,\n",
    "#                         figsize=(H/patch_size, W/patch_size), # no. of patches per col & row\n",
    "#                         sharex=True,\n",
    "#                         sharey=True)\n",
    "\n",
    "# for i, patch_i in enumerate(range(0, H, patch_size)):\n",
    "#     for j, patch_j in enumerate(range(0, W, patch_size)):\n",
    "#         axs[i][j].imshow(img_permute[patch_i:patch_i+patch_size, patch_j:patch_j+patch_size, :])\n",
    "#         axs[i][j].set_xticks([])\n",
    "#         axs[i][j].set_yticks([])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image patch embeddnigs (2D image linear projection layer) / Implementation of Equation 1\n",
    "class Patch_Embed_Layer(nn.Module):\n",
    "    def __init__(self, in_channels, num_patch, patch_size, embed_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding_layer = nn.Sequential(\n",
    "            # extract the 2D feature maps (learnable patches)\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=num_patch,\n",
    "                      kernel_size=patch_size,\n",
    "                      stride=patch_size,\n",
    "                      padding=0),\n",
    "            # flatten the feature maps\n",
    "            nn.Flatten(start_dim=2,\n",
    "                       end_dim=3),\n",
    "            #  linear projection to create patch embedings\n",
    "            nn.Linear(in_features=14*14, out_features=embed_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # concat class embedding with patch embedding\n",
    "        # have to generate batchwise class tokens!!!\n",
    "        # x = torch.concat([self.class_embedding, self.patch_embedding_layer(x)], dim=1)\n",
    "        \n",
    "        # generate positional embeddng for given sequence\n",
    "        return self.patch_embedding_layer(x)\n",
    "\n",
    "# BATCH_SIZE = 2\n",
    "# embed_size = 768\n",
    "# num_patch = 196\n",
    "# patch_size = 16\n",
    "# # print(f\"Image shape {img.unsqueeze(0).shape}\")\n",
    "# dm_img = torch.rand(size=(BATCH_SIZE, 3, 224, 224))\n",
    "# print(dm_img.shape)\n",
    "# patch_embd = Patch_Embed_Layer(num_patch, patch_size, embed_size)\n",
    "# patch_embd_output = patch_embd(dm_img)\n",
    "# print(f\"Image patches embeding shape {patch_embd_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MSA Block / Implementation of Equation 2 without skip connection\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, attn_dropout=0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # qkv for input in the attention block\n",
    "        self.query = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.key = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # layer norm\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embed_size)\n",
    "        \n",
    "        # attention block\n",
    "        self.self_attention_layer = nn.MultiheadAttention(embed_dim=embed_size,\n",
    "                                                          num_heads=num_heads,\n",
    "                                                          dropout=attn_dropout,\n",
    "                                                          batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # get the attn output and ignore attn output weights\n",
    "        return self.self_attention_layer(query=q,\n",
    "                                         key=k,\n",
    "                                         value=v,\n",
    "                                         need_weights=False)[0]\n",
    "\n",
    "# msa_layer = MultiheadAttention(4, 768)\n",
    "# msa_output = msa_layer(patch_embd_output)\n",
    "# msa_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Block / Implementation of Equation 3 without skip connection\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size, # hidden size D from Table 1\n",
    "                 mlp_size:int=3072, # from Table 1 of ViT-Base\n",
    "                 dropout:float=0.1): # from Table 3 of ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embed_size)\n",
    "\n",
    "        # mlp layer as specified in ViT Paper\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(embed_size, mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(mlp_size, embed_size),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layer(self.layer_norm(x))\n",
    "    \n",
    "# mlp_layer = MultiLayerPerceptron(embed_size)\n",
    "# output_mlp_layer = mlp_layer(msa_output)\n",
    "# output_mlp_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer encoder block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, mlp_size, dropout_attn, dropout) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-headed attention block with layer norm\n",
    "        self.msa_block = MultiheadAttention(num_heads,\n",
    "                                            embed_size,\n",
    "                                            dropout_attn)\n",
    "        \n",
    "        # MLP Block with layer norm\n",
    "        self.mlp_block = MultiLayerPerceptron(embed_size,\n",
    "                                              mlp_size,\n",
    "                                              dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # msa with layer norm + skip connection\n",
    "        x = self.msa_block(x) + x\n",
    "        # mlp with layer norm + skip connection\n",
    "        return self.mlp_block(x) + x\n",
    "    \n",
    "# encoder = EncoderBlock(4, embed_size, 3072, 0.1)\n",
    "# import timeit\n",
    "# start = timeit.default_timer()\n",
    "# encoder(patch_embd_output).shape\n",
    "# print(timeit.default_timer() - start)\n",
    "\n",
    "# summary(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ViT Architecture (ViT Base Model)\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 224  # image resolution\n",
    "IMG_CHANNELS = 3  # image channels\n",
    "NUM_CLASSES = 3 # output labels\n",
    "PATCH_SIZE = 16  # dimension of the image patches\n",
    "NUM_PATCH = IMAGE_SIZE**2 // PATCH_SIZE**2  # number of image patches\n",
    "D_MODEL = 768  # patch embedding dimension throughout ViT\n",
    "NUM_HEADS = 12 # number of heads for multiheaded attention block\n",
    "NUM_LAYERS = 12 # number of encoder blocks in ViT\n",
    "MLP_SIZE = 3072 # size of the MLP block in Encoder\n",
    "DROPOUT_EMBEDS = 0.1 # dropout of the patch embeddings\n",
    "DROPOUT_MLP = 0.1 # dropout of the MLP block in Encoder\n",
    "DROPOUT_ATTN = 0 # dropout of the MSA block in Encoder\n",
    "\n",
    "class ViTBase(nn.Module):\n",
    "    def __init__(self, in_channels, num_patch, patch_size, embed_size, num_heads, num_layers, mlp_size, dropout_embeds, dropout_mlp, dropout_attn, out_features) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image patch embeddings\n",
    "        self.patch_embedding = Patch_Embed_Layer(in_channels, num_patch, patch_size, embed_size)\n",
    "        \n",
    "        # Class token embedding\n",
    "        self.class_embedding = nn.Parameter(torch.randn(size=(1, 1, embed_size), requires_grad=True))\n",
    "        \n",
    "        # Postition embeddings of flatten patches\n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1, num_patch+1, embed_size), requires_grad=True))\n",
    "\n",
    "        # Embeddings dropout\n",
    "        self.embedding_dropout = nn.Dropout(p=dropout_embeds)\n",
    "        \n",
    "        # Encoder block layers\n",
    "        self.encoder_blocks = nn.Sequential(\n",
    "            *[EncoderBlock(num_heads, embed_size, mlp_size, dropout_attn, dropout_mlp) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Classification head for image classification\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_size),\n",
    "            nn.Linear(embed_size, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]  # Get batch dimension\n",
    "\n",
    "        # Create the image patches' embeddings\n",
    "        patch_embeddings = self.patch_embedding(x)\n",
    "        \n",
    "        # Create the output tokens similar to BERT's class token for the batch\n",
    "        class_embedding = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Create the complete sequence of embeddings with class token, patch and positional embeddings\n",
    "        x_embedded = torch.concat([class_embedding, patch_embeddings], dim=1) + self.position_embedding\n",
    "\n",
    "        # Run embedding dropout (Appendix B.1 in ViT Paper)\n",
    "        x_embedded_drpout = self.embedding_dropout(x_embedded)\n",
    "        \n",
    "        # Pass the input through the Encoder layers\n",
    "        encoder_output = self.encoder_blocks(x_embedded_drpout)\n",
    "        \n",
    "        # Get the class token embeddings\n",
    "        class_token = encoder_output[:, 0,:]\n",
    "        \n",
    "        # Get the output labels from class token\n",
    "        return self.classifier_head(class_token)\n",
    "\n",
    "dm_img = torch.rand(size=(BATCH_SIZE, 3, 224, 224))\n",
    "# print(dm_img.shape)\n",
    "vit_model = ViTBase(IMG_CHANNELS, NUM_PATCH, PATCH_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, MLP_SIZE, DROPOUT_EMBEDS, DROPOUT_MLP, DROPOUT_ATTN, NUM_CLASSES)\n",
    "# vit_model(dm_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "ViTBase (ViTBase)                                                 [32, 3, 224, 224]    [32, 3]              152,064              True\n",
       "├─Patch_Embed_Layer (patch_embedding)                             [32, 3, 224, 224]    [32, 196, 768]       --                   True\n",
       "│    └─Sequential (patch_embedding_layer)                         [32, 3, 224, 224]    [32, 196, 768]       --                   True\n",
       "│    │    └─Conv2d (0)                                            [32, 3, 224, 224]    [32, 196, 14, 14]    150,724              True\n",
       "│    │    └─Flatten (1)                                           [32, 196, 14, 14]    [32, 196, 196]       --                   --\n",
       "│    │    └─Linear (2)                                            [32, 196, 196]       [32, 196, 768]       151,296              True\n",
       "├─Dropout (embedding_dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "├─Sequential (encoder_blocks)                                     [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    └─EncoderBlock (0)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (1)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (2)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (3)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (4)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (5)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (6)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (7)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (8)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (9)                                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (10)                                          [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─EncoderBlock (11)                                          [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadAttention (msa_block)                        [32, 197, 768]       [32, 197, 768]       4,133,376            True\n",
       "│    │    └─MultiLayerPerceptron (mlp_block)                      [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "├─Sequential (classifier_head)                                    [32, 768]            [32, 3]              --                   True\n",
       "│    └─LayerNorm (0)                                              [32, 768]            [32, 768]            1,536                True\n",
       "│    └─Linear (1)                                                 [32, 768]            [32, 3]              2,307                True\n",
       "=================================================================================================================================================\n",
       "Total params: 106,746,055\n",
       "Trainable params: 106,746,055\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.44\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 4696.38\n",
       "Params size (MB): 312.98\n",
       "Estimated Total Size (MB): 5028.63\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=vit_model, \n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "\n",
    "# vit_weights = models.ViT_B_16_Weights.DEFAULT\n",
    "# vit_transfermodel = models.vit_b_16(weights=vit_weights)\n",
    "\n",
    "# vit_transfermodel.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
